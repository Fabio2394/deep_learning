{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xgvfAxDotxp"
      },
      "source": [
        "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
        "<table align=\"center\">\n",
        " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/RNN/IMBD_RNN.ipynb\">\n",
        "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/IMBD_RNN.ipynb\">\n",
        "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKZOThY0otxs"
      },
      "source": [
        "# Classification Example\n",
        " Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9hG4SJeCotxs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def show_loss_accuracy_evolution(history):\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Sparse Categorical Crossentropy')\n",
        "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
        "    ax1.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy')\n",
        "    ax2.grid()\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1-9F55Botxu"
      },
      "source": [
        "## The Dataset: The IMDB dataset\n",
        "We’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The  parameter `num_words` controls how many words different we want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O2y_L6wCotxu",
        "outputId": "3ca65e1e-d4de-4418-b1a9-54e8453b5d2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "# Importa el dataset IMDB (reseñas de películas) desde la librería Keras de TensorFlow. (el dataset imdb no es un DataFrame de pandas)\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Define que se van a considerar sólo las 10,000 palabras más frecuentes del dataset.\n",
        "num_words = 10000\n",
        "\n",
        "# Carga el dataset: se divide en datos de entrenamiento y prueba.\n",
        "# Cada reseña está codificada como una lista de enteros (cada entero representa una palabra).\n",
        "# También se cargan las etiquetas, que indican si la reseña es positiva (1) o negativa (0).\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Muestra por consola la primera reseña del conjunto de entrenamiento, como una lista de índices de palabras.\n",
        "print(train_data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1DuGJ-Ypotxu",
        "outputId": "ffc3ad65-d331-4a50-8fe2-9b9cbbf95ebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Transform word_id to word and reverse\n",
        "\n",
        "# Carga el diccionario original que mapea palabras a enteros (índices), basado en frecuencia.\n",
        "word2int = imdb.get_word_index()\n",
        "\n",
        "# Ajusta los índices para dejar espacio a tokens especiales agregando 3 a cada valor original.\n",
        "# Por ejemplo, si 'película' era 5, ahora será 8.\n",
        "word2int = {w: i+3 for w, i in word2int.items()}\n",
        "\n",
        "# Se agregan tokens especiales al diccionario:\n",
        "word2int[\"<PAD>\"] = 0       # Usado para rellenar secuencias hasta que tengan la misma longitud.\n",
        "word2int[\"<START>\"] = 1     # Marca el inicio de una reseña.\n",
        "word2int[\"<UNK>\"] = 2       # Representa palabras desconocidas/no incluidas en el vocabulario.\n",
        "word2int[\"<UNUSED>\"] = 3    # Reservado para usos futuros o pruebas.\n",
        "\n",
        "# Se crea el diccionario inverso: de índice a palabra, para poder decodificar las reseñas.\n",
        "int2word = {i: w for w, i in word2int.items()}\n",
        "\n",
        "# Se actualiza el número de palabras para reflejar que ahora hay 3 tokens más.\n",
        "num_words = num_words + 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z_zBneaotxv"
      },
      "source": [
        "For transforming an id-sequence to a phrase use get_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w2BMDhiDotxv",
        "outputId": "44b6cff0-b77b-4f27-e969-1cd552a4c635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Define una función que recibe una reseña codificada (lista de enteros) y un diccionario índice->palabra.\n",
        "def get_words(sentence, int2word):\n",
        "    # Convierte cada entero en su palabra correspondiente (usa '<UNK>' si no se encuentra) y las une en una sola cadena.\n",
        "    return ' '.join([int2word.get(i, '<UNK>') for i in sentence])\n",
        "\n",
        "# Llama la función para decodificar la primera reseña del conjunto de entrenamiento.\n",
        "get_words(train_data[0], int2word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuame8U9otxw"
      },
      "source": [
        "# MLP model\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "You need to convert your raw text to an appropriate input to a sequential model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wyhld57eotxw",
        "outputId": "a783b15f-a50a-4735-c8b7-8c22654a8761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "def vectorize_text_sentence(text, word2int):\n",
        "    tokens = text.split(' ')\n",
        "    tokens_id = [word2int.get(tk,2) for tk in tokens]\n",
        "    return tokens_id\n",
        "\n",
        "text = get_words(train_data[0], int2word)\n",
        "print(text)\n",
        "print(vectorize_text_sentence(text, word2int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wclzSthcotxx"
      },
      "source": [
        "We are going to use a bag of words model. BoW is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the Each key is the word, and each value is the frequency of occurrences of that word in the given text document.\n",
        "\n",
        "- **Input document**: `\"John likes to watch movies Mary likes movies too\"`\n",
        "- **BoW**: `{'John': 0.11, 'likes': 0.22, 'to': 0.11, 'watch': 0.11, 'movies': 0.22, 'Mary': 0.11, 'too': 0.11}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yFGMTXFxotxx",
        "outputId": "9b8888eb-6a91-4bc9-c30f-04dcc00f9e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_example John likes to watch movies Mary likes movies too\n",
            "text splitted ['John', 'likes', 'to', 'watch', 'movies', 'Mary', 'likes', 'movies', 'too']\n",
            "bag_of_words {'John': 0.1111111111111111, 'likes': 0.2222222222222222, 'to': 0.1111111111111111, 'watch': 0.1111111111111111, 'movies': 0.2222222222222222, 'Mary': 0.1111111111111111, 'too': 0.1111111111111111}\n",
            "bag_of_words norm=False {'John': 1, 'likes': 2, 'to': 1, 'watch': 1, 'movies': 2, 'Mary': 1, 'too': 1}\n",
            "bag_of_words with indexes {308: 1, 1232: 2, 8: 1, 106: 1, 102: 2, 1083: 1, 99: 1}\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_bag_of_words(sequence, norm=True):\n",
        "    word_count = Counter(sequence)\n",
        "    if norm:\n",
        "        total = sum(word_count.values())\n",
        "        word_freq = {w: n / total for w, n in word_count.items()}\n",
        "        return word_freq\n",
        "    else:\n",
        "        return dict(word_count.items())\n",
        "\n",
        "\n",
        "text_example = \"John likes to watch movies Mary likes movies too\"\n",
        "print('text_example', text_example)\n",
        "text_sequence = text_example.split()\n",
        "print('text splitted', text_sequence)\n",
        "bag_of_words = get_bag_of_words(text_sequence)\n",
        "print('bag_of_words', bag_of_words)\n",
        "print('bag_of_words norm=False', get_bag_of_words(text_sequence, norm=False))\n",
        "print(\n",
        "    'bag_of_words with indexes', {\n",
        "        word2int[w.lower()]: p\n",
        "        for w, p in get_bag_of_words(text_sequence, norm=False).items()\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ivc0ugkotxx"
      },
      "source": [
        "After that, we convert every BoW to a vector of `dim=num_words` with `vectorize_sequences`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a814roPnotxx",
        "outputId": "afed8503-a6f4-4b0e-b72f-70c5ffbbb7c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 10003), (25000,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def vectorize_sequence(sequence, num_words, norm=True):\n",
        "    vec = np.zeros(num_words)\n",
        "    bow = get_bag_of_words(sequence, norm)\n",
        "    for w, freq in bow.items():\n",
        "        if w < num_words:\n",
        "            vec[w] = freq\n",
        "    return vec\n",
        "\n",
        "\n",
        "def vectorize_sequences(sequences, num_words=num_words, norm=True):\n",
        "    \"\"\"Creates an all-zero matrix of shape (len(sequences), num_words)\"\"\"\n",
        "    results = np.zeros((len(sequences), num_words))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, :] = vectorize_sequence(sequence, num_words, norm)\n",
        "    return results\n",
        "\n",
        "\n",
        "x_train = vectorize_sequences(train_data, num_words=num_words)\n",
        "x_test = vectorize_sequences(test_data, num_words=num_words)\n",
        "y_train =np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-KUk-CYotxy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbP2x3oUotxy"
      },
      "source": [
        "## Define and train a model\n",
        "Define, compile and fit your sequential model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdREyEfwotxy"
      },
      "outputs": [],
      "source": [
        "model = ...\n",
        "history = model.fit(x_train, y_train, validation_split=0.2, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Pi1vCSotxy"
      },
      "outputs": [],
      "source": [
        "show_loss_accuracy_evolution(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7575vsycotxy"
      },
      "source": [
        "### Evaluate the model\n",
        "You need to obtain a Test Accuracy > 0.85. Try to get more than 0.9!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHaky6dgotxy"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib01RL1_otxy"
      },
      "source": [
        "### Making predictioins with new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF7M1TY8otxz"
      },
      "outputs": [],
      "source": [
        "reviews = ['the film was really bad and i am very disappointed',\n",
        "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
        "           'this film was just brilliant']\n",
        "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
        "             for review in reviews]\n",
        "\n",
        "x_pred = vectorize_sequences(sequences, num_words=num_words)\n",
        "np.round(model.predict(x_pred), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtZj-oLWotxz"
      },
      "source": [
        "# RNN model\n",
        "\n",
        "Lets use a recurrent neural network and compare results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brIsRxJBotxz"
      },
      "source": [
        "### Simple RNN model\n",
        "\n",
        "There are three built-in RNN layers in Keras:\n",
        "\n",
        "1. [`keras.layers.SimpleRNN`](https://keras.io/api/layers/recurrent_layers/simple_rnn/), a fully-connected RNN where the output from previous\n",
        "timestep is to be fed to next timestep.\n",
        "\n",
        "```python\n",
        "tf.keras.layers.SimpleRNN(\n",
        "    units,\n",
        "    dropout=0.0,\n",
        "    recurrent_dropout=0.0,\n",
        "    return_sequences=False,\n",
        "    return_state=False,\n",
        "    go_backwards=False,\n",
        "    stateful=False,\n",
        ")\n",
        "````\n",
        "\n",
        "2. [`keras.layers.GRU`](https://keras.io/api/layers/recurrent_layers/gru/), first proposed in\n",
        "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
        "```python\n",
        "tf.keras.layers.GRU(\n",
        "    units,\n",
        "    dropout=0.0,\n",
        "    recurrent_dropout=0.0,\n",
        "    return_sequences=False,\n",
        "    return_state=False,\n",
        "    go_backwards=False,\n",
        "    stateful=False,\n",
        ")\n",
        "```\n",
        "\n",
        "3. [`keras.layers.LSTM`](https://keras.io/api/layers/recurrent_layers/lstm/), first proposed in\n",
        "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
        "```python\n",
        "tf.keras.layers.LSTM(\n",
        "    units,\n",
        "    dropout=0.0,\n",
        "    recurrent_dropout=0.0,\n",
        "    return_sequences=False,\n",
        "    return_state=False,\n",
        "    go_backwards=False,\n",
        "    stateful=False,\n",
        ")\n",
        "````\n",
        "For more information, see the\n",
        "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In sequence classification we are going to use the **many-to-one** architecture with default parameter `return_sequences=False`.\n",
        "\n",
        "The shape of the output  for this architecture  is `(batch_size, units)`.\n",
        "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
        "\n",
        "Lets see one some examples for understanding the input/output dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFDpHoExotxz"
      },
      "outputs": [],
      "source": [
        "# dims of input: [batch, timesteps, features]\n",
        "inputs = tf.random.normal([32, 10, 4])\n",
        "print('input dim (batch, timesteps, feature): ', inputs.shape)\n",
        "# return_sequences=False, return_state=False\n",
        "lstm = tf.keras.layers.LSTM(units= 2)\n",
        "output = lstm(inputs)\n",
        "print('return_state=False output shape: ',output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFanjehzotxz"
      },
      "source": [
        "### Deep RNN\n",
        "We can stack multiple layers of RNNs on top of each other. Each hidden state is continuously passed to both the next time step of the current layer and the current time step of the next layer.\n",
        "\n",
        "For stack another RNN layer to an existing one, we need to use the states with `return_sequences=True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGiG4Ijvotxz"
      },
      "outputs": [],
      "source": [
        "## We can modify the input vector before the rnn cell with TimeDistributed\n",
        "timesteps = 10\n",
        "features = 8 # dimension of the innput of every cell\n",
        "\n",
        "#Shape [batch, timesteps, features]\n",
        "inputs = tf.keras.Input(shape=(timesteps, features), name='input')\n",
        "lstm_1 = layers.LSTM(64, return_sequences=True, name='lstm_1')(inputs)\n",
        "lstm_2 = layers.LSTM(64, return_sequences=True, name='lstm_2')(lstm_1)\n",
        "# last lstm layer depends in [one to many or  many to many]\n",
        "lstm_3 = layers.LSTM(64, return_sequences=False, name='lstm_3')(lstm_2)\n",
        "model = keras.Model(inputs=inputs, outputs=lstm_3, name='rnn_example')\n",
        "#print(model.summary())\n",
        "inputs = tf.random.normal([32, timesteps, features])\n",
        "print(model(inputs).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPew4RDgotxz"
      },
      "source": [
        "### Bidirectional RNNs\n",
        "\n",
        "For sequences other than time series (e.g. text), it is often the case that a RNN model\n",
        "can perform better if it not only processes sequence from start to end, but also\n",
        "backwards. For example, to predict the next word in a sentence, it is often useful to\n",
        "have the context around the word, not only just the words that come before it.\n",
        "\n",
        "Keras provides an easy API for you to build such bidirectional RNNs: the\n",
        "`keras.layers.Bidirectional` wrapper.\n",
        "\n",
        "[link to documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVpoudB1otx0"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "# If you crete a second layer you must set return_sequences=True\n",
        "model.add(\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(timesteps, features))\n",
        ")\n",
        "# Second Bidirectional layer\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "# Output\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtR2OMymotx0"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08kMdEOVotx0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "num_words = 2000\n",
        "((train_data, train_labels), (test_data, test_labels)\n",
        " ) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "#  limit the data for class time\n",
        "'''size = 15000\n",
        "(train_data, train_labels), (test_data, test_labels) = (\n",
        "    (train_data[:size], train_labels[:size]), (test_data[:size], test_labels[:size]))\n",
        "'''\n",
        "# Transform word_id to word and reverse\n",
        "word2int = imdb.get_word_index()\n",
        "word2int = {w: i+3 for w, i in word2int.items()}\n",
        "word2int[\"<PAD>\"] = 0\n",
        "word2int[\"<START>\"] = 1\n",
        "word2int[\"<UNK>\"] = 2\n",
        "word2int[\"<UNUSED>\"] = 3\n",
        "int2word = {i: w for w, i in word2int.items()}\n",
        "num_words = num_words+3\n",
        "\n",
        "print(train_data.shape, test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJP2OvPjotx0"
      },
      "source": [
        "#### Data Preprocessing\n",
        "\n",
        "For data preprocessing we first use [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences):\n",
        "```python\n",
        "tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences, maxlen=None, dtype='int32', padding='pre',\n",
        "    truncating='pre', value=0.0\n",
        ")\n",
        "```\n",
        "- **padding**:\t'pre' or 'post' (optional, defaults to 'pre'): pad either before or after each sequence.\n",
        "- **truncating**:\tString, 'pre' or 'post' (optional, defaults to 'pre'): remove values from sequences larger than maxlen, either at the beginning or at the end of the sequences.\n",
        "\n",
        "\n",
        "Our RNN will take sequences of constant length. In our case this length is the `maxlen`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOuzIBPsotx0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "input_seq = [1, 2, 3]\n",
        "max_len = 5\n",
        "print('input sequence: ', input_seq)\n",
        "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len)\n",
        "print('input sequence with padding: ', pad_seq)\n",
        "\n",
        "input_seq = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "max_len = 5\n",
        "print('input sequence: ', input_seq)\n",
        "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len)\n",
        "print('input sequence with padding: ', pad_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haFbeKL5otx0"
      },
      "outputs": [],
      "source": [
        "input_seq = [1, 2, 3]\n",
        "max_len = 5\n",
        "print('input sequence: ', input_seq)\n",
        "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len, padding='post')\n",
        "print('input sequence with padding: ', pad_seq)\n",
        "\n",
        "input_seq = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "max_len = 5\n",
        "print('input sequence: ', input_seq)\n",
        "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len, truncating='post')\n",
        "print('input sequence with padding: ', pad_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoMyqjhRotx1"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "x_train_seq = sequence.pad_sequences(train_data, maxlen=max_len, truncating='post', padding='post')\n",
        "x_test_seq = sequence.pad_sequences(test_data, maxlen=max_len, truncating='post', padding='post')\n",
        "\n",
        "print('train shape:', x_train_seq.shape)\n",
        "print('test shape:', x_test_seq.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH-kuEnIotx1"
      },
      "source": [
        "### Create the RNN model\n",
        "\n",
        "For the input of the first rnn layer we need a tensor of `(timesteps, features)` or `(batchsize, timesteps, features)`. We have a matrix of sentences of `(train_size, max_len)`. Every sentence is a  `max_len`, we need to convert it to a sentence of one-hot vectors of dim `(max_len, num_words)`.\n",
        "For get the one-hot encoding of every sequence we are going to use:\n",
        "\n",
        "```python\n",
        "layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
        "  input_length=max_len, embeddings_initializer='identity', trainable=False)\n",
        "```\n",
        "\n",
        "This layer converts the input tensor `(batch_size, max_len)` to one-hot encoded sequences `(batch_size, max_len, num_words)`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3Ha4jG-Fotx1"
      },
      "outputs": [],
      "source": [
        "seq = np.array([[[0, 1, 2, 2, 0]]])\n",
        "print(seq, seq.shape)\n",
        "layers.Embedding(input_dim=3, output_dim=3,\n",
        "                 input_length=5, embeddings_initializer='identity',\n",
        "                 trainable=False)(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "660Wyw1Aotx1"
      },
      "source": [
        "### RNN model\n",
        "Use `keras.layers.SimpleRNN`,  `keras.layers.GRU`,  `keras.layers.LSTM` or `keras.layers.Bidirectional`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_vTmTNNotx1"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
        "## one-hot encoding\n",
        "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
        "                           input_length=max_len, embeddings_initializer='identity',\n",
        "                           trainable=False))\n",
        "## complete the model with recurrent layers\n",
        "#model.add(...)\n",
        "model.add(layers.SimpleRNN(16, return_sequences=False))\n",
        "## add binary classification output\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH3tHKctotx6"
      },
      "outputs": [],
      "source": [
        "## set the loss and see the results\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "epochs = 5\n",
        "history = model.fit(x_train_seq, train_labels,\n",
        "                    validation_split=0.1, epochs=epochs,\n",
        "                    batch_size=256)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cADytfNotx6"
      },
      "outputs": [],
      "source": [
        "show_loss_accuracy_evolution(history)\n",
        "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csJ8V3Lxotx6"
      },
      "outputs": [],
      "source": [
        "def show_errors(x_test, model, labels, int2word, n_samples=10):\n",
        "    preds = 1.0 * (model.predict(x_test).flatten() > 0.5)\n",
        "    bad_pred_inds = np.where(preds != labels)[0]\n",
        "    n_samples = min(len(bad_pred_inds), n_samples)\n",
        "    samples_inds = np.random.choice(bad_pred_inds, n_samples)\n",
        "    for ind in samples_inds:\n",
        "        print('Predicted : {0}, real : {1}, lenght: {2}'.format(\n",
        "            int(preds[ind]), labels[ind], len(test_data[ind])))\n",
        "        print(get_words(test_data[ind], int2word))\n",
        "        print()\n",
        "    return\n",
        "\n",
        "show_errors(x_test_seq, model, test_labels, int2word, n_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVyqHx5hotx6"
      },
      "source": [
        "#### Making predictioins with new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89KsbZWfotx6"
      },
      "outputs": [],
      "source": [
        "reviews = ['the film was really bad and i am very disappointed',\n",
        "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
        "           'this film was just brilliant',\n",
        "           'the film is not good',\n",
        "           'the film is not bad',\n",
        "          'the movie is not bad I like it']\n",
        "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
        "             for review in reviews]\n",
        "\n",
        "## Padding the sequences\n",
        "x_pred  = sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')# ...\n",
        "\n",
        "np.round(model.predict(x_pred), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGBDm1oCotx6"
      },
      "outputs": [],
      "source": [
        "1.0*(model.predict(x_pred) > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eam5-hUNotx6"
      },
      "source": [
        "### GRU model\n",
        "Use `keras.layers.GRU`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odz-tEACotx7"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
        "## one-hot encoding\n",
        "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
        "                           input_length=max_len, embeddings_initializer='identity',\n",
        "                           trainable=False))\n",
        "\n",
        "## complete the model with recurrent layers\n",
        "model.add(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8gi9-6Aotx7"
      },
      "outputs": [],
      "source": [
        "## set the loss and see the results\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=...,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "epochs = 5\n",
        "history = model.fit(x_train_seq, train_labels,\n",
        "                    validation_split=0.1, epochs=epochs,\n",
        "                    batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq1xzOO5otx7"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3e9v7y5otx7"
      },
      "source": [
        "### LSTM model\n",
        "Use `keras.layers.LSTM`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtxFq5y5otx7"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
        "## one-hot encoding\n",
        "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
        "                           input_length=max_len, embeddings_initializer='identity',\n",
        "                           trainable=False))\n",
        "\n",
        "## complete the model with recurrent layers\n",
        "model.add(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg3gfuWzotx7"
      },
      "outputs": [],
      "source": [
        "## set the loss and see the results\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=...,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "epochs = 5\n",
        "history = model.fit(x_train_seq, train_labels,\n",
        "                    validation_split=0.1, epochs=epochs,\n",
        "                    batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L61TwSa5otx7"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRhhHbfhotx7"
      },
      "source": [
        "### Deep model\n",
        "Use `keras.layers.SimpleRNN`,  `keras.layers.GRU`,  `keras.layers.LSTM` or `keras.layers.Bidirectional`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhHM1TAjotx8"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
        "## one-hot encoding\n",
        "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
        "                           input_length=max_len, embeddings_initializer='identity',\n",
        "                           trainable=False))\n",
        "\n",
        "## complete the model with recurrent layers\n",
        "model.add(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Clnxzi47otx8"
      },
      "outputs": [],
      "source": [
        "## set the loss and see the results\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=...,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "epochs = 5\n",
        "history = model.fit(x_train_seq, train_labels,\n",
        "                    validation_split=0.1, epochs=epochs,\n",
        "                    batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qb8v2Aeotx8"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd204D_lotx8"
      },
      "source": [
        "### Bidirectional model\n",
        "Use `keras.layers.SimpleRNN`,  `keras.layers.GRU`,  `keras.layers.LSTM` with `keras.layers.Bidirectional`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i88336xuotx8"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
        "## one-hot encoding\n",
        "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
        "                           input_length=max_len, embeddings_initializer='identity',\n",
        "                           trainable=False))\n",
        "\n",
        "## complete the model with recurrent layers\n",
        "model.add(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLtOHgh7otx9"
      },
      "outputs": [],
      "source": [
        "## set the loss and see the results\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=...,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "epochs = 5\n",
        "history = model.fit(x_train_seq, train_labels,\n",
        "                    validation_split=0.1, epochs=epochs,\n",
        "                    batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGgBgLdHotx9"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1trvENWJotx9"
      },
      "source": [
        "#### Making predictioins with new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arqZaqPFotx-"
      },
      "outputs": [],
      "source": [
        "reviews = ['the film was really bad and i am very disappointed',\n",
        "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
        "           'this film was just brilliant',\n",
        "           'the film is not good',\n",
        "           'the film is not bad',\n",
        "          'the movie is not bad I like it']\n",
        "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
        "             for review in reviews]\n",
        "\n",
        "## Padding the sequences\n",
        "x_pred  = sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')# ...\n",
        "\n",
        "np.round(model.predict(x_pred), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dHnMy1cotx-"
      },
      "outputs": [],
      "source": [
        "show_errors(x_test_seq, model, test_labels, int2word, n_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zglpdnCDotx-"
      },
      "source": [
        "###  Use a convolutional network instead of a RNN\n",
        "\n",
        "```python\n",
        "tf.keras.layers.Conv1D(\n",
        "    filters, kernel_size\n",
        ")\n",
        "```\n",
        "\n",
        "```python\n",
        "tf.keras.layers.MaxPool1D(\n",
        "    pool_size=2\n",
        ")\n",
        "```\n",
        "\n",
        "```python\n",
        "tf.keras.layers.Flatten()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjjnv6GOotx-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "num_words = 2000\n",
        "((train_data, train_labels), (test_data, test_labels)\n",
        " ) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "#  limit the data for class time\n",
        "# Transform word_id to word and reverse\n",
        "word2int = imdb.get_word_index()\n",
        "word2int = {w: i+3 for w, i in word2int.items()}\n",
        "word2int[\"<PAD>\"] = 0\n",
        "word2int[\"<START>\"] = 1\n",
        "word2int[\"<UNK>\"] = 2\n",
        "word2int[\"<UNUSED>\"] = 3\n",
        "int2word = {i: w for w, i in word2int.items()}\n",
        "num_words = num_words+3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X2POGlpotx-"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "x_train_seq = sequence.pad_sequences(train_data, maxlen=max_len, truncating='post', padding='post')\n",
        "x_test_seq = sequence.pad_sequences(test_data, maxlen=max_len, truncating='post', padding='post')\n",
        "\n",
        "print('train shape:', x_train_seq.shape)\n",
        "print('test shape:', x_test_seq.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRl1KGZIotx_"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
        "## one-hot encoding\n",
        "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
        "                           input_length=max_len, embeddings_initializer='identity',\n",
        "                           trainable=False))\n",
        "\n",
        "\n",
        "model.add(layers.Conv1D(32, 3, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(2))\n",
        "\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Conv1D(32, 3, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(2))\n",
        "\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyr3LrRFotx_"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "epochs = 5\n",
        "history = model.fit(x_train_seq, train_labels,\n",
        "                    validation_split=0.1, epochs=epochs,\n",
        "                    batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txyiau3fotx_"
      },
      "outputs": [],
      "source": [
        "reviews = ['the film was really bad and i am very disappointed',\n",
        "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
        "           'this film was just brilliant',\n",
        "           'the film is not good',\n",
        "           'the film is not bad']\n",
        "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
        "             for review in reviews]\n",
        "\n",
        "## Padding the sequences\n",
        "x_pred  = sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')# ...\n",
        "\n",
        "np.round(model.predict(x_pred), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTQ8FC4jotx_"
      },
      "outputs": [],
      "source": [
        "show_errors(x_test_seq, model, test_labels, int2word, n_samples=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}